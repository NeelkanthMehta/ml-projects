{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stanford Dogs\n",
    "\n",
    "### A notebook by [Ashish Rai](https://github.com/darthv115/)\n",
    "\n",
    "**It is recommended to view this notebook in [nbviewer](http://nbviewer.jupyter.org/github/darthv115/ml-projects/blob/master/stanford_dogs/ipython_notebooks/Stanford_Dogs.ipynb) for the best viewing experience.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "- [Introduction](#intro)\n",
    "\n",
    "\n",
    "- [Licence](#licence)\n",
    "\n",
    "\n",
    "- [Required Libraries](#reqd_libs)\n",
    "\n",
    "\n",
    "- [Data Preprocessing](#data_preprocessing)\n",
    "\n",
    "\n",
    "- [Approaches](#approaches)\n",
    "\n",
    "    - [Approach 1](#approach1)\n",
    "    \n",
    "    - [Approach 2](#approach2)    \n",
    "    \n",
    "    - [Approach 3](#approach3)\n",
    "    \n",
    "\n",
    "- [Instructions to see results](#see_results)\n",
    "\n",
    "\n",
    "- [Conclusions](#conclusions)\n",
    "\n",
    "\n",
    "- [What to do next](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>  \n",
    "[[go back to top]](#toc)\n",
    "\n",
    "This notebook is related to the classification task of the Stanford Dogs dataset.  \n",
    "Direclty from the [paper](http://people.csail.mit.edu/khosla/papers/fgvc2011.pdf) which proposed this dataset:\n",
    "> The Stanford Dogs dataset contains 22,000 annotated images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization.  This dataset is extremely challenging due to a variety of reasons. First, being a fine-grained categorization problem, there is little inter-class variation. For example the bassethound and bloodhound share very similar facial characteristics but differ significantly in their color, while the Japanese spaniel and papillion share very similar color but greatly differ in their facial characteristics. Second, there is very large intra-class variation. The images show that dogs within a class could have different ages (e.g. beagle), poses (e.g. blenheim spaniel), occlusion/self-occlusion and even color (e.g. Shih-tzu). Furthermore, compared to other animal datasets that tend to exist in natural scenes, a large proportion of the images contain humans and are taken in manmade environments leading to greater background variation. The aforementioned reasons make this an extremely challenging dataset.\n",
    "\n",
    "From a [paper](http://webcache.googleusercontent.com/search?q=cache:oyIrmrXwzHQJ:vision.stanford.edu/teaching/cs231n/reports/fcdh_FinalReport.pdf) which cannot be found on the web anymore except for the cache servers of Google:\n",
    "> Different approaches have been explored for FGIC. A traditional method is to use **different descriptor extraction algorithms**, and to run a linear classifier on the extracted features. Khosla et. al, who created the Stanford Dogs dataset, were able to achieve 22% accuracy using SIFT descriptors for classification on Stanford Dogs. The current method for achieving the highest accuracy (52%) on the Stanford Dogs dataset is by using **Selective Pooling Vectors**, which encodes descriptors into vectors, and selects only those that are below a certain threshold of quantization error, with respect to the codebook which is used to approximate the nonlinear function f used to determine the classification likelihoods of various classes.\n",
    "\n",
    "> Another approach is to “localize” various landmarks inside the particular class, and to co-register these landmarks and perform comparisons on them. Both supervised and unsupervised learning methods have been applied here. Unsupervised learning methods have been developed to **learn “template” shape patterns** which commonly re-occur in all images being categorized, and have been able to show up to 38% accuracy on the Stanford Dogs dataset. One paper utilizes R-CNNs to categorize the CalTech-UCSD Birds, by leveraging CNNs in both the part localization and image classification portions of the tasks. This was extended to two-level attention models, where the images are split into patches, with one level determining the relevance of the patch, and the next performing the actual classification. Approaches involving **gnostic fields** have achieved some of the highest levels of accuracy on the Stanford Dogs dataset (47% accuracy), using pattern-detection units and image descriptors to create a size and shape-invariant model.\n",
    "\n",
    "You can get the dataset from [here](http://vision.stanford.edu/aditya86/ImageNetDogs/).\n",
    "\n",
    "I tried using the deep CNNs to extract the features and make a robust model for classification.\n",
    "\n",
    "**This notebook is mostly about the various approaches I took towards solving this problem and the reasons behind taking those approaches.**\n",
    "\n",
    "I won't delve deep into the code which can be viewed [here](https://github.com/darthv115/Machine-Learning-and-Data-Science-Projects/tree/master/Stanford_Dogs). This is about the higher level decisions taken by me and reasons behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Licence <a class=\"anchor\" id=\"licence\"></a>  \n",
    "[[go back to top]](#toc)  \n",
    "\n",
    "Please refer to the repository <a href=\"https://github.com/darthv115/Machine-Learning-and-Data-Science-Projects/blob/master/Licence.md\">Licence</a> file for the licenses and usage terms for the instructional material and code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Required Libraries <a class=\"anchor\" id=\"reqd_libs\"></a>  \n",
    "[[go back to top]](#toc)  \n",
    "\n",
    "You need to have python installed on your system. Refer to this [link](https://www.python.org/downloads/) to get python on your system.\n",
    "\n",
    "Required Python packages:\n",
    "- theano\n",
    "- keras\n",
    "- numpy\n",
    "- matplotlib\n",
    "- Pillow\n",
    "\n",
    "If you have a GPU supported system, you can get the Nvidia Cuda Toolkit from [here](https://developer.nvidia.com/cuda-toolkit). You can then configure theano to use the GPU for which the procedure has been explained [here](http://deeplearning.net/software/theano/tutorial/using_gpu.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preprocessing <a class='anchor' id='#data_preprocessing'></a>\n",
    "[[go back to top]](#toc)  \n",
    "\n",
    "The entire process of data preprocessing has been explained in this [notebook](https://github.com/darthv115/Machine-Learning-and-Data-Science-Projects/blob/master/Stanford_Dogs/ipython_notebooks/Data_Preprocessing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Approaches <a class='anchor' id='approaches'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Approach 1 <a class='anchor' id='approach1'></a>\n",
    "[[go back to top]](#toc)  \n",
    "\n",
    "Script: [**<code>scripts/first_model.py</code>**](https://github.com/darthv115/ml-projects/tree/master/stanford_dogs/scripts/first_model.py)\n",
    "\n",
    "So, by now I have learnt that you always start small in machine learning and see if your approach is in the correct direction. Meaning to check if you are getting any progress.\n",
    "\n",
    "So, I created a smaller dataset of about 12000 images to start off. I made a shallow CNN with 3 conv-pool and 1 FC (relu) layer with softmax classifier. Ran it for like 5 epochs and it showed progress.\n",
    "\n",
    "Results:\n",
    "![title](../plots/approach1/first_try.png)\n",
    "<center><img src='../plots/approach1/first_try.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Approach 2 <a class='anchor' id='#approach2'></a>\n",
    "[[go back to top]](#toc)  \n",
    "\n",
    "Script for the final attempt: [**<code>scripts/pretrained_network.py</code>**](https://github.com/darthv115/ml-projects/tree/master/stanford_dogs/scripts/pretrained_network.py)\n",
    "\n",
    "Because of this dataset similar in some respects to the ImageNet dataset and also due to the lack of computational resources, instead of training my convnet from scratch I decided to use other networks which were already trained on the ImageNet dataset. Also, because I was using a subset of the entire dataset [12000 images - 100 images per class] (also, due to lack of computational resources), I thought using a not-very-deep network would be helpful because then overfitting kicks in (due to lack of data). So, I decided to go with **VGG16 network with 'imagenet' weights**.\n",
    "\n",
    "I created a bottleneck model of VGG16 (excluding the fully connected layers). Coupled with 2 FC layer (relu) and a sigmoid classifier, I started tuning the parameters.\n",
    "\n",
    "I did a forward pass through the trained VGG16 net and saved the weights, then used them to fine tune the model.\n",
    "\n",
    "The attempts were made with respect to **setting the learning rate** and **the choice of classifier**.\n",
    "\n",
    "Model layout:\n",
    "**<pre>VGG16 : Flatten : Dense(256, 'relu', dropout(0.5)) : Dense(120, 'sigmoid')</pre>**\n",
    "\n",
    "#### Attempt 1\n",
    "Learning Rate: 5e-04 (fixed)  \n",
    "Epochs: 20\n",
    "\n",
    "![title](../plots/approach2/slow_bottleneck_vgg16_epoch_20.png)\n",
    "\n",
    "Keeping the learning rate fixed leads to oscillations. So, made the learning rate variable.\n",
    "\n",
    "#### Attempt 2\n",
    "\n",
    "Learning Rate: 5e-04 with a decay of 1e-05 every epoch  \n",
    "Epochs: 20\n",
    "\n",
    "![title](../plots/approach2/bn_vgg16_var_lr_epoch_20.png)\n",
    "\n",
    "Epochs: 50\n",
    "\n",
    "![title](../plots/approach2/bn_vgg16_var_lr_epoch_50.png)\n",
    "\n",
    "\n",
    "Overfitting kicks in. To tackle this, increased dropout of dense (relu) to 0.6.\n",
    "\n",
    "#### Attempt 3\n",
    "\n",
    "Model Layout:\n",
    "**<pre>VGG16 : Flatten : Dense(256, 'relu', dropout(0.5)) : Dense(120, 'sigmoid')</pre>**\n",
    "\n",
    "Learning Rate: 5e-04 with decay of 2.5e-05 per epoch.\n",
    "Epochs: 20\n",
    "\n",
    "![title](../plots/approach2/bn_vgg16_var_lr_dp_0.6_epoch_20.png)\n",
    "\n",
    "This took care of overfitting very well.\n",
    "\n",
    "#### Attempt 4\n",
    "\n",
    "Also tried updating the initial learning rate.\n",
    "Learning Rate: 6e-04 with decay of 1e-05 per epoch\n",
    "Epochs: 20\n",
    "\n",
    "![title](../plots/approach2/bn_vgg16_var_lr_0.0006_epoch_20.png)\n",
    "\n",
    "\n",
    "#### Attempt 5\n",
    "\n",
    "Change the classifier activation to **softmax**.\n",
    "\n",
    "Learning rate: 6e-04 with decay of 1e-05 per epoch\n",
    "Epochs: 50\n",
    "![title](../plots/approach2/bn_vgg16_sofmx_var_lr_6e-04_ep_50.png)\n",
    "\n",
    "Due to some reasons, I got better performance with sigmoid classifier than with softmax classifier.\n",
    "Will update this when I figure out the reason.\n",
    "\n",
    "#### Attempt 6\n",
    "\n",
    "Back to **sigmoid** classifier.\n",
    "\n",
    "Epochs: 50\n",
    "![title](../plots/approach2/bn_vgg16_sigmoid_step_decay_0.5_lr_5e-04_ep_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Approach 3 <a class='anchor' id='approach3'></a>\n",
    "[[go back to top]](#toc)  \n",
    "\n",
    "Script for the final attempt: [**<code>scripts/regularized_small_vgg_bottleneck.py</code>**](https://github.com/darthv115/ml-projects/tree/master/stanford_dogs/scripts/regularized_small_vgg_bottleneck.py)\n",
    "\n",
    "So, I was trying to improve the performance of my network by employing some kind of regualization as the model was overfitting. I came to know that Global Average Pooling is one such method. So, I stacked a GAP layer on top of the VGG16 net with its output going in a softmax classifier layer. The reason why Global Average Pooling layer works better than having stacks of dense layers (with dropout) is explained in this [paper](https://arxiv.org/pdf/1312.4400.pdf).\n",
    "\n",
    "\n",
    "#### Attempt 1\n",
    "\n",
    "Model layout:\n",
    "(No Regularization)\n",
    "\n",
    "**<pre>VGG16 : GAP2D : Dense(512, 'relu') : Dense(120, 'softmax')</pre>**\n",
    "\n",
    "\n",
    "I made the last 3 conv layers trainable and trained the model with a very low learning rate (SGD, lr = 1e-04)\n",
    "\n",
    "Fine tuning the unregularized model:\n",
    "![title](../plots/approach3/full_vgg_unreg.png)\n",
    "\n",
    "To take this further, I made the next 3 conv layers trainable too. Same learning rate as before.\n",
    "\n",
    "Results:\n",
    "![title](../plots/approach3/full_vgg_unreg_2.png)\n",
    "\n",
    "Even though, I got the best training accuracy of about 96 % and validation accuracy of 56 %, as you can see, the issue of overfitting is still there. And in the last plot, the validation loss increases due to overfitting.\n",
    "\n",
    "To deal with this, I had two ideas:\n",
    "- Add regularization to the dense as well as conv layers\n",
    "- Drop some of the conv-pool layers (because less data)\n",
    "\n",
    "\n",
    "#### Attempt 2\n",
    "\n",
    "Added regularization to the final dense (relu) layer.\n",
    "\n",
    "Model layout:\n",
    "**<pre>VGG16 : GAP2D : Dense(512, 'relu', l2(0.01)) : Dense(120, 'softmax')</pre>**\n",
    "\n",
    "Fine-tuning the model:\n",
    "![title](../plots/approach3/full_vgg_reg_var_lr_5e-04.png)\n",
    "\n",
    "However, I did not gain much.\n",
    "\n",
    "\n",
    "#### Attempt 3\n",
    "\n",
    "Removed the last conv-pool layer (3 conv & 1 pool) of VGG16.\n",
    "Added regularization to the last 3 conv layers.\n",
    "\n",
    "Model layout:\n",
    "**<pre>VGG16 : GAP2D : Dense(512, 'relu', l2(0.01)) : Dense(120, 'softmax')</pre>**\n",
    "\n",
    "Training bottleneck model:\n",
    "![title](../plots/approach3/small_vgg_bn_50ep.png)\n",
    "\n",
    "\n",
    "While fine tuning, I added regularization to the last conv-pool layer (11:14). I took it one step further by training the second last set of conv-pool layers (7:11). Aslo added small regularization to them. Increased the value of regularization parameters.\n",
    "\n",
    "Model layout:\n",
    "**<pre>VGG16 (layers [7:11] l2(0.005) [11:14], l2(0.01)) : GAP2D : Dense(512, 'relu', l2(0.05)) : Dense(120, 'softmax')</pre>**\n",
    "\n",
    "Due to some constraints, I could only train for 10 and 15 epochs.\n",
    "\n",
    "Fine tuning: 10 epochs\n",
    "![title](../plots/approach3/small_full_reg.png)\n",
    "\n",
    "\n",
    "Further fine tuning: 15 epochs\n",
    "![title](../plots/approach3/small_full_reg_2.png)\n",
    "\n",
    "So, finally\n",
    "**<pre>Train Accuracy: 67 %</pre>**\n",
    "**<pre>Validation Accuracy: 43.5 %</pre>**\n",
    "\n",
    "**<pre>The main improvement here is that the model is no longer overfitting!<pre>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Insrtructions to see results <a class='anchor' id='see_results'></a>\n",
    "\n",
    "1. Clone the git repository from [here](https://github.com/darthv115/ml-projects/).  \n",
    "2. Change drive to the stanford_dogs directory.\n",
    "3. Run the test.py script in the scripts directory.\n",
    "<pre><code>python test.py</code></pre>\n",
    "\n",
    "Alternatively, if you just want to clone the stanford_dogs subdirectory, you can do sparse checkout, for which info can be found [here](https://git-scm.com/docs/git-read-tree#_sparse_checkout) and [here](http://stackoverflow.com/questions/600079/how-do-i-clone-a-subdirectory-only-of-a-git-repository/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What to do next <a class='anchor' id='next'></a>\n",
    "\n",
    "There are a couple of things which come to my mind thinking of what to do next. Here are a few:\n",
    "\n",
    "- First of all, I used a subset of the dataset without any data augmentation methods. So, using the entire dataset or using a suitable data augmentation scheme will improve the model's capacity to generalize and overall will make it more possible to extract more features without overfitting the model.\n",
    "- A multi-scale architecture might help the model become more size and shape invariant and overall improve the model accuracy [[citation](http://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Wang_Learning_Fine-grained_Image_2014_CVPR_paper.html)].\n",
    "- A two level architecture with the help of RNNs, one for selecting the local features and the other for classification can help extract more peculiar features from the almost similar classes. This is like making a two level attention model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
